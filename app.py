import os
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from openai import OpenAI
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# ==============================
# åˆå§‹åŒ–åº”ç”¨
# ==============================
app = FastAPI(title="è¡Œä¸šçŸ¥è¯†é—®ç­”åŠ©æ‰‹ ğŸš€")

# è·¨åŸŸé…ç½®ï¼ˆå…è®¸å‰ç«¯è®¿é—®ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==============================
# åŸºç¡€é…ç½®
# ==============================
UPLOAD_DIR = "uploads"
VECTOR_DIR = "vectorstore"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(VECTOR_DIR, exist_ok=True)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"),
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
# å­˜å‚¨ç”¨æˆ·å¯¹è¯å†å²
conversation_histories = {}
# åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹ï¼ˆDashScope çš„ embedding æ¨¡å‹ï¼‰
embeddings = DashScopeEmbeddings(
    model="text-embedding-v1", dashscope_api_key=os.getenv("OPENAI_API_KEY"))

# ==============================
# æ–‡ä»¶ä¸Šä¼ æ¥å£
# ==============================


@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """
    ä¸Šä¼ æ–‡ä»¶ -> è§£æ -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å‚¨
    """
    file_path = os.path.join(UPLOAD_DIR, str(file.filename))
    with open(file_path, "wb") as f:
        f.write(await file.read())

    # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
    filename = file.filename or ""
    if filename.endswith(".pdf"):
        loader = PyPDFLoader(file_path)
    elif filename.endswith(".docx"):
        loader = Docx2txtLoader(file_path)
    else:
        loader = TextLoader(file_path, encoding="utf-8")

    # è¯»å–æ–‡æ¡£
    docs = loader.load()
    # åˆ†å—
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800, chunk_overlap=100)
    chunks = splitter.split_documents(docs)
    # å»ºç«‹æˆ–æ›´æ–°å‘é‡åº“
    if os.path.exists(VECTOR_DIR) and os.listdir(VECTOR_DIR):
        # å·²å­˜åœ¨å‘é‡åº“ -> è¿½åŠ æ–°æ–‡æ¡£
        db = FAISS.load_local(VECTOR_DIR, embeddings,
                              allow_dangerous_deserialization=True)
        db.add_documents(chunks)
    else:
        # ç¬¬ä¸€æ¬¡åˆ›å»ºå‘é‡åº“
        db = FAISS.from_documents(chunks, embeddings)
    db.save_local(VECTOR_DIR)

    return {"message": f"{file.filename} ä¸Šä¼ æˆåŠŸå¹¶å·²å…¥åº“ï¼", "chunks": len(chunks)}

# ==============================
# QA é—®ç­”æ¥å£
# ==============================


@app.post("/qa")
async def qa(query: str = Form(...), session_id: str = Form(...)):
    """
    ä»å‘é‡åº“ä¸­æ£€ç´¢æœ€ç›¸ä¼¼å†…å®¹ -> ç»“åˆä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
    session_id: ç”¨æˆ·ä¼šè¯IDï¼Œç”¨äºä¿å­˜å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
    """
    # è·å–ä¼šè¯å†å²
    history = conversation_histories.get(session_id, [])
    # åŠ å…¥æœ¬è½®ç”¨æˆ·é—®é¢˜
    history.append({"role": "user", "content": query})
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å‘é‡åº“
    if not os.path.exists(VECTOR_DIR) or not os.listdir(VECTOR_DIR):
        return {"error": "å½“å‰æš‚æ— çŸ¥è¯†åº“ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ï¼"}
    # åŠ è½½å‘é‡åº“
    db = FAISS.load_local(VECTOR_DIR, embeddings,
                          allow_dangerous_deserialization=True)
    # æ£€ç´¢ç›¸ä¼¼å†…å®¹
    docs = db.similarity_search(query, k=3)
    context = "\n\n".join([d.page_content for d in docs])
    # æ„é€ æç¤º
    prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¡Œä¸šçŸ¥è¯†åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æ ¹æ®ä¸‹åˆ—å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
è¦æ±‚ï¼š
1. ä»…ä½¿ç”¨å‚è€ƒèµ„æ–™ä¸­çš„ä¿¡æ¯å›ç­”ï¼Œä¸å¾—å‡­ç©ºæ¨æµ‹ã€‚
2. å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å›ç­”â€œèµ„æ–™ä¸­æœªæåŠâ€ã€‚
3. å›ç­”å°½é‡ç®€æ˜ã€ç›´æ¥ï¼Œé¿å…æ— å…³ä¿¡æ¯ã€‚

å‚è€ƒèµ„æ–™ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š
{query}

è¯·ç»™å‡ºç­”æ¡ˆï¼š
"""
    prompt_messages = [
        {"role": "user", "content": prompt}
    ] + history
    completion = client.chat.completions.create(
        model="qwen-plus",
        messages=prompt_messages
    )

    answer = completion.choices[0].message.content
    # ä¿å­˜æ¨¡å‹å›ç­”åˆ°å†å²
    history.append({"role": "assistant", "content": answer})
    conversation_histories[session_id] = history  # æ›´æ–°ä¼šè¯
    return {"query": query, "answer": answer, "source_count": len(docs), "session_id": session_id}

# è¿”å›å½“å‰ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨


@app.get("/files")
async def list_files():
    """
    è¿”å›å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
    """
    files = os.listdir(UPLOAD_DIR)
    return {"files": files}

# ==============================
# é¦–é¡µæµ‹è¯•
# ==============================


@app.get("/")
async def root():
    return {"message": "è¡Œä¸šçŸ¥è¯†é—®ç­”åŠ©æ‰‹å·²å¯åŠ¨ ğŸš€", "functions": ["upload", "qa"]}
